{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TabularDataset(pd.concat([X, y], axis=1))\n",
    "# Define the target variable and the features\n",
    "label = 'MedHouseVal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240421_203038/\"\n",
      "Presets specified: ['medium_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'auto_stack': False, 'included_model_types': ['NN_TORCH']}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': None,\n",
      " 'auto_stack': False,\n",
      " 'calibrate': 'auto',\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'included_model_types': ['NN_TORCH'],\n",
      " 'keep_only_best': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': None,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'refit_full': False,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 4}\n",
      "========================================\n",
      "Saving AutogluonModels/ag-20240421_203038/learner.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/predictor.pkl\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240421_203038/\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 23.0.0: Fri Sep 15 14:41:43 PDT 2023; root:xnu-10002.1.13~1/RELEASE_ARM64_T6000\n",
      "Disk Space Avail:   745.56 GB / 994.66 GB (75.0%)\n",
      "Train Data Rows:    20640\n",
      "Train Data Columns: 8\n",
      "Label Column: MedHouseVal\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3925.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.32 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\t\tSkipping CategoryFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float64', 'float') : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t\t\t0.0s = Fit runtime\n",
      "\t\t\t8 features in original data used to generate 8 features in processed data.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('float64', 'float') : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 8 | ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t8 features in original data used to generate 8 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.32 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'pinball_loss'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving AutogluonModels/ag-20240421_203038/learner.pkl\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 18576, Val Rows: 2064\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Saving AutogluonModels/ag-20240421_203038/utils/data/X.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/utils/data/y.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/utils/data/X_val.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/utils/data/y_val.pkl\n",
      "Included models: ['NN_TORCH'] (Specified by `included_model_types`, all other model types will be skipped)\n",
      "Model configs that will be trained (in order):\n",
      "\tNeuralNetTorch: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_fit': {'quantile_levels': array([0.25, 0.5 , 0.75])}}\n",
      "Fitting 1 L1 models ...\n",
      "Fitting model: NeuralNetTorch ...\n",
      "\tDropped 0 of 8 features.\n",
      "\tFitting NeuralNetTorch with 'num_gpus': 0, 'num_cpus': 10\n",
      "Tabular Neural Network treats features as the following types:\n",
      "{\n",
      "    \"continuous\": [\n",
      "        \"HouseAge\",\n",
      "        \"Latitude\",\n",
      "        \"Longitude\"\n",
      "    ],\n",
      "    \"skewed\": [\n",
      "        \"MedInc\",\n",
      "        \"AveRooms\",\n",
      "        \"AveBedrms\",\n",
      "        \"Population\",\n",
      "        \"AveOccup\"\n",
      "    ],\n",
      "    \"onehot\": [],\n",
      "    \"embed\": [],\n",
      "    \"language\": [],\n",
      "    \"bool\": []\n",
      "}\n",
      "\n",
      "\n",
      "Training data for TabularNeuralNetTorchModel has: 18576 examples, 8 features (8 vector, 0 embedding)\n",
      "Training on CPU\n",
      "Neural network architecture:\n",
      "EmbedNet(\n",
      "  (main_block): Sequential(\n",
      "    (0): Linear(in_features=8, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Dropout(p=0.1, inplace=False)\n",
      "    (9): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (10): ReLU()\n",
      "    (11): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n",
      "Training tabular neural network for up to 500 epochs...\n",
      "Epoch 1 (Update 145).\tTrain loss: 0.3956, Val pinball_loss: -0.2516, Best Epoch: 1\n",
      "Epoch 2 (Update 290).\tTrain loss: 0.2167, Val pinball_loss: -0.1879, Best Epoch: 2\n",
      "Epoch 3 (Update 435).\tTrain loss: 0.196, Val pinball_loss: -0.1802, Best Epoch: 3\n",
      "Epoch 4 (Update 580).\tTrain loss: 0.1886, Val pinball_loss: -0.1765, Best Epoch: 4\n",
      "Epoch 5 (Update 725).\tTrain loss: 0.1847, Val pinball_loss: -0.1725, Best Epoch: 5\n",
      "Epoch 6 (Update 870).\tTrain loss: 0.1811, Val pinball_loss: -0.1695, Best Epoch: 6\n",
      "Epoch 7 (Update 1015).\tTrain loss: 0.1787, Val pinball_loss: -0.1656, Best Epoch: 7\n",
      "Epoch 8 (Update 1160).\tTrain loss: 0.1764, Val pinball_loss: -0.1653, Best Epoch: 8\n",
      "Epoch 9 (Update 1305).\tTrain loss: 0.1736, Val pinball_loss: -0.1623, Best Epoch: 9\n",
      "Epoch 10 (Update 1450).\tTrain loss: 0.172, Val pinball_loss: -0.1618, Best Epoch: 10\n",
      "Epoch 11 (Update 1595).\tTrain loss: 0.1699, Val pinball_loss: -0.1588, Best Epoch: 11\n",
      "Epoch 12 (Update 1740).\tTrain loss: 0.1695, Val pinball_loss: -0.1612, Best Epoch: 11\n",
      "Epoch 13 (Update 1885).\tTrain loss: 0.1674, Val pinball_loss: -0.1574, Best Epoch: 13\n",
      "Epoch 14 (Update 2030).\tTrain loss: 0.1664, Val pinball_loss: -0.1575, Best Epoch: 13\n",
      "Epoch 15 (Update 2175).\tTrain loss: 0.1654, Val pinball_loss: -0.1547, Best Epoch: 15\n",
      "Epoch 16 (Update 2320).\tTrain loss: 0.1643, Val pinball_loss: -0.1541, Best Epoch: 16\n",
      "Epoch 17 (Update 2465).\tTrain loss: 0.163, Val pinball_loss: -0.1559, Best Epoch: 16\n",
      "Epoch 18 (Update 2610).\tTrain loss: 0.1626, Val pinball_loss: -0.1537, Best Epoch: 18\n",
      "Epoch 19 (Update 2755).\tTrain loss: 0.1618, Val pinball_loss: -0.1542, Best Epoch: 18\n",
      "Epoch 20 (Update 2900).\tTrain loss: 0.1604, Val pinball_loss: -0.1528, Best Epoch: 20\n",
      "Epoch 21 (Update 3045).\tTrain loss: 0.1601, Val pinball_loss: -0.1527, Best Epoch: 21\n",
      "Epoch 22 (Update 3190).\tTrain loss: 0.1604, Val pinball_loss: -0.1509, Best Epoch: 22\n",
      "Epoch 23 (Update 3335).\tTrain loss: 0.1584, Val pinball_loss: -0.1507, Best Epoch: 23\n",
      "Epoch 24 (Update 3480).\tTrain loss: 0.1578, Val pinball_loss: -0.1504, Best Epoch: 24\n",
      "Epoch 25 (Update 3625).\tTrain loss: 0.1565, Val pinball_loss: -0.1491, Best Epoch: 25\n",
      "Epoch 26 (Update 3770).\tTrain loss: 0.1566, Val pinball_loss: -0.1504, Best Epoch: 25\n",
      "Epoch 27 (Update 3915).\tTrain loss: 0.1552, Val pinball_loss: -0.1493, Best Epoch: 25\n",
      "Epoch 28 (Update 4060).\tTrain loss: 0.1547, Val pinball_loss: -0.149, Best Epoch: 28\n",
      "Epoch 29 (Update 4205).\tTrain loss: 0.1543, Val pinball_loss: -0.1486, Best Epoch: 29\n",
      "Epoch 30 (Update 4350).\tTrain loss: 0.1535, Val pinball_loss: -0.1471, Best Epoch: 30\n",
      "Epoch 31 (Update 4495).\tTrain loss: 0.1536, Val pinball_loss: -0.1471, Best Epoch: 30\n",
      "Epoch 32 (Update 4640).\tTrain loss: 0.1531, Val pinball_loss: -0.1464, Best Epoch: 32\n",
      "Epoch 33 (Update 4785).\tTrain loss: 0.1517, Val pinball_loss: -0.1478, Best Epoch: 32\n",
      "Epoch 34 (Update 4930).\tTrain loss: 0.1521, Val pinball_loss: -0.1475, Best Epoch: 32\n",
      "Epoch 35 (Update 5075).\tTrain loss: 0.1522, Val pinball_loss: -0.1489, Best Epoch: 32\n",
      "Epoch 36 (Update 5220).\tTrain loss: 0.1505, Val pinball_loss: -0.1456, Best Epoch: 36\n",
      "Epoch 37 (Update 5365).\tTrain loss: 0.1504, Val pinball_loss: -0.1474, Best Epoch: 36\n",
      "Epoch 38 (Update 5510).\tTrain loss: 0.1502, Val pinball_loss: -0.1467, Best Epoch: 36\n",
      "Epoch 39 (Update 5655).\tTrain loss: 0.1502, Val pinball_loss: -0.1447, Best Epoch: 39\n",
      "Epoch 40 (Update 5800).\tTrain loss: 0.1492, Val pinball_loss: -0.1448, Best Epoch: 39\n",
      "Epoch 41 (Update 5945).\tTrain loss: 0.1499, Val pinball_loss: -0.1449, Best Epoch: 39\n",
      "Epoch 42 (Update 6090).\tTrain loss: 0.149, Val pinball_loss: -0.1469, Best Epoch: 39\n",
      "Epoch 43 (Update 6235).\tTrain loss: 0.1477, Val pinball_loss: -0.145, Best Epoch: 39\n",
      "Epoch 44 (Update 6380).\tTrain loss: 0.1477, Val pinball_loss: -0.1459, Best Epoch: 39\n",
      "Epoch 45 (Update 6525).\tTrain loss: 0.1479, Val pinball_loss: -0.1451, Best Epoch: 39\n",
      "Epoch 46 (Update 6670).\tTrain loss: 0.147, Val pinball_loss: -0.1435, Best Epoch: 46\n",
      "Epoch 47 (Update 6815).\tTrain loss: 0.1466, Val pinball_loss: -0.1454, Best Epoch: 46\n",
      "Epoch 48 (Update 6960).\tTrain loss: 0.1465, Val pinball_loss: -0.1434, Best Epoch: 48\n",
      "Epoch 49 (Update 7105).\tTrain loss: 0.1461, Val pinball_loss: -0.1433, Best Epoch: 49\n",
      "Epoch 50 (Update 7250).\tTrain loss: 0.1455, Val pinball_loss: -0.1437, Best Epoch: 49\n",
      "Epoch 51 (Update 7395).\tTrain loss: 0.1457, Val pinball_loss: -0.1432, Best Epoch: 51\n",
      "Epoch 52 (Update 7540).\tTrain loss: 0.1457, Val pinball_loss: -0.1438, Best Epoch: 51\n",
      "Epoch 53 (Update 7685).\tTrain loss: 0.1448, Val pinball_loss: -0.1424, Best Epoch: 53\n",
      "Epoch 54 (Update 7830).\tTrain loss: 0.1444, Val pinball_loss: -0.1415, Best Epoch: 54\n",
      "Epoch 55 (Update 7975).\tTrain loss: 0.1436, Val pinball_loss: -0.1429, Best Epoch: 54\n",
      "Epoch 56 (Update 8120).\tTrain loss: 0.1437, Val pinball_loss: -0.1433, Best Epoch: 54\n",
      "Epoch 57 (Update 8265).\tTrain loss: 0.1436, Val pinball_loss: -0.1416, Best Epoch: 54\n",
      "Epoch 58 (Update 8410).\tTrain loss: 0.1428, Val pinball_loss: -0.1435, Best Epoch: 54\n",
      "Epoch 59 (Update 8555).\tTrain loss: 0.1435, Val pinball_loss: -0.1421, Best Epoch: 54\n",
      "Epoch 60 (Update 8700).\tTrain loss: 0.1427, Val pinball_loss: -0.1411, Best Epoch: 60\n",
      "Epoch 61 (Update 8845).\tTrain loss: 0.1427, Val pinball_loss: -0.1412, Best Epoch: 60\n",
      "Epoch 62 (Update 8990).\tTrain loss: 0.1417, Val pinball_loss: -0.1421, Best Epoch: 60\n",
      "Epoch 63 (Update 9135).\tTrain loss: 0.1417, Val pinball_loss: -0.1406, Best Epoch: 63\n",
      "Epoch 64 (Update 9280).\tTrain loss: 0.1419, Val pinball_loss: -0.1428, Best Epoch: 63\n",
      "Epoch 65 (Update 9425).\tTrain loss: 0.1413, Val pinball_loss: -0.1408, Best Epoch: 63\n",
      "Epoch 66 (Update 9570).\tTrain loss: 0.1405, Val pinball_loss: -0.1409, Best Epoch: 63\n",
      "Epoch 67 (Update 9715).\tTrain loss: 0.1423, Val pinball_loss: -0.1415, Best Epoch: 63\n",
      "Epoch 68 (Update 9860).\tTrain loss: 0.1408, Val pinball_loss: -0.1402, Best Epoch: 68\n",
      "Epoch 69 (Update 10005).\tTrain loss: 0.1406, Val pinball_loss: -0.1398, Best Epoch: 69\n",
      "Epoch 70 (Update 10150).\tTrain loss: 0.1402, Val pinball_loss: -0.1426, Best Epoch: 69\n",
      "Epoch 71 (Update 10295).\tTrain loss: 0.1409, Val pinball_loss: -0.1419, Best Epoch: 69\n",
      "Epoch 72 (Update 10440).\tTrain loss: 0.1395, Val pinball_loss: -0.1413, Best Epoch: 69\n",
      "Epoch 73 (Update 10585).\tTrain loss: 0.1386, Val pinball_loss: -0.1411, Best Epoch: 69\n",
      "Epoch 74 (Update 10730).\tTrain loss: 0.1379, Val pinball_loss: -0.1401, Best Epoch: 69\n",
      "Epoch 75 (Update 10875).\tTrain loss: 0.1396, Val pinball_loss: -0.1385, Best Epoch: 75\n",
      "Epoch 76 (Update 11020).\tTrain loss: 0.1382, Val pinball_loss: -0.1382, Best Epoch: 76\n",
      "Epoch 77 (Update 11165).\tTrain loss: 0.1386, Val pinball_loss: -0.1399, Best Epoch: 76\n",
      "Epoch 78 (Update 11310).\tTrain loss: 0.1385, Val pinball_loss: -0.1397, Best Epoch: 76\n",
      "Epoch 79 (Update 11455).\tTrain loss: 0.1377, Val pinball_loss: -0.1374, Best Epoch: 79\n",
      "Epoch 80 (Update 11600).\tTrain loss: 0.138, Val pinball_loss: -0.1395, Best Epoch: 79\n",
      "Epoch 81 (Update 11745).\tTrain loss: 0.1375, Val pinball_loss: -0.1396, Best Epoch: 79\n",
      "Epoch 82 (Update 11890).\tTrain loss: 0.1371, Val pinball_loss: -0.14, Best Epoch: 79\n",
      "Epoch 83 (Update 12035).\tTrain loss: 0.1368, Val pinball_loss: -0.1383, Best Epoch: 79\n",
      "Epoch 84 (Update 12180).\tTrain loss: 0.1365, Val pinball_loss: -0.1383, Best Epoch: 79\n",
      "Epoch 85 (Update 12325).\tTrain loss: 0.1373, Val pinball_loss: -0.1393, Best Epoch: 79\n",
      "Epoch 86 (Update 12470).\tTrain loss: 0.136, Val pinball_loss: -0.1384, Best Epoch: 79\n",
      "Epoch 87 (Update 12615).\tTrain loss: 0.1357, Val pinball_loss: -0.1394, Best Epoch: 79\n",
      "Epoch 88 (Update 12760).\tTrain loss: 0.1363, Val pinball_loss: -0.1383, Best Epoch: 79\n",
      "Epoch 89 (Update 12905).\tTrain loss: 0.1351, Val pinball_loss: -0.1379, Best Epoch: 79\n",
      "Epoch 90 (Update 13050).\tTrain loss: 0.1355, Val pinball_loss: -0.1398, Best Epoch: 79\n",
      "Epoch 91 (Update 13195).\tTrain loss: 0.1355, Val pinball_loss: -0.1403, Best Epoch: 79\n",
      "Epoch 92 (Update 13340).\tTrain loss: 0.135, Val pinball_loss: -0.1392, Best Epoch: 79\n",
      "Epoch 93 (Update 13485).\tTrain loss: 0.1352, Val pinball_loss: -0.1388, Best Epoch: 79\n",
      "Epoch 94 (Update 13630).\tTrain loss: 0.1354, Val pinball_loss: -0.1387, Best Epoch: 79\n",
      "Epoch 95 (Update 13775).\tTrain loss: 0.1337, Val pinball_loss: -0.1382, Best Epoch: 79\n",
      "Epoch 96 (Update 13920).\tTrain loss: 0.1336, Val pinball_loss: -0.1389, Best Epoch: 79\n",
      "Epoch 97 (Update 14065).\tTrain loss: 0.1336, Val pinball_loss: -0.1381, Best Epoch: 79\n",
      "Epoch 98 (Update 14210).\tTrain loss: 0.1333, Val pinball_loss: -0.1378, Best Epoch: 79\n",
      "Epoch 99 (Update 14355).\tTrain loss: 0.1335, Val pinball_loss: -0.1396, Best Epoch: 79\n",
      "Best model found on Epoch 79 (Update 11455). Val pinball_loss: -0.13739738534192628\n",
      "Saving AutogluonModels/ag-20240421_203038/models/NeuralNetTorch/model.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/utils/attr/NeuralNetTorch/y_pred_proba_val.pkl\n",
      "\t-0.1374\t = Validation score   (-pinball_loss)\n",
      "\t25.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Saving AutogluonModels/ag-20240421_203038/models/trainer.pkl\n",
      "Loading: AutogluonModels/ag-20240421_203038/utils/attr/NeuralNetTorch/y_pred_proba_val.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}, 'ag_args_fit': {'quantile_levels': array([0.25, 0.5 , 0.75])}}\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tDropped 0 of 3 features.\n",
      "\tDropped 0 of 3 features.\n",
      "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 10\n",
      "Saving AutogluonModels/ag-20240421_203038/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Loading: AutogluonModels/ag-20240421_203038/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "\tDropped 0 of 3 features.\n",
      "Ensemble size: 1\n",
      "Ensemble indices: [0]\n",
      "Ensemble weights: \n",
      "[1.]\n",
      "Saving AutogluonModels/ag-20240421_203038/models/WeightedEnsemble_L2/utils/oof.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/models/WeightedEnsemble_L2/model.pkl\n",
      "\t-0.1374\t = Validation score   (-pinball_loss)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Saving AutogluonModels/ag-20240421_203038/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/models/trainer.pkl\n",
      "AutoGluon training complete, total runtime = 26.05s ... Best model: \"WeightedEnsemble_L2\"\n",
      "Loading: AutogluonModels/ag-20240421_203038/models/trainer.pkl\n",
      "Loading: AutogluonModels/ag-20240421_203038/utils/data/X_val.pkl\n",
      "Loading: AutogluonModels/ag-20240421_203038/models/NeuralNetTorch/model.pkl\n",
      "Loading: AutogluonModels/ag-20240421_203038/models/WeightedEnsemble_L2/model.pkl\n",
      "Loading: AutogluonModels/ag-20240421_203038/utils/data/y_val.pkl\n",
      "Loading: AutogluonModels/ag-20240421_203038/models/WeightedEnsemble_L2/model.pkl\n",
      "Conformity scores being computed to calibrate model: WeightedEnsemble_L2\n",
      "Saving AutogluonModels/ag-20240421_203038/models/WeightedEnsemble_L2/model.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/models/trainer.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/learner.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/predictor.pkl\n",
      "Saving AutogluonModels/ag-20240421_203038/__version__ with contents \"0.8.2\"\n",
      "Saving AutogluonModels/ag-20240421_203038/metadata.json\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240421_203038/\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogluon.tabular.predictor.predictor.TabularPredictor at 0x2c0363310>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the predictor\n",
    "predictor = TabularPredictor(\n",
    "    label=label,\n",
    "    problem_type='quantile',  # Specify quantile regression\n",
    "    quantile_levels=[0.25, 0.5, 0.75],  # Median\n",
    "    verbosity=4  # High verbosity for detailed output\n",
    ")\n",
    "\n",
    "# Train the predictor\n",
    "predictor.fit(\n",
    "    data,\n",
    "    included_model_types = [\"NN_TORCH\"],\n",
    "    presets='medium_quality',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading: AutogluonModels/ag-20240421_202731/models/NeuralNetTorch/model.pkl\n",
      "Loading: AutogluonModels/ag-20240421_202731/models/WeightedEnsemble_L2/model.pkl\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.405855445326752"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((y_pred[0.5] - y) / y).abs().median() * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
