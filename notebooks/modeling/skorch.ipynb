{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m0.9917\u001b[0m        \u001b[32m1.0012\u001b[0m  0.8254\n",
      "      2        \u001b[36m0.9904\u001b[0m        \u001b[32m1.0012\u001b[0m  0.8415\n",
      "      3        \u001b[36m0.9904\u001b[0m        1.0012  0.8539\n",
      "      4        \u001b[36m0.9904\u001b[0m        1.0012  0.8639\n",
      "      5        0.9904        1.0012  0.8053\n",
      "      6        0.9904        1.0012  0.8536\n",
      "      7        0.9904        1.0012  0.8350\n",
      "      8        0.9904        1.0012  0.8565\n",
      "      9        0.9904        1.0012  0.7985\n",
      "     10        0.9904        1.0012  0.8222\n",
      "[[0.05384421]\n",
      " [0.05384421]\n",
      " [0.05384421]\n",
      " [0.05384421]\n",
      " [0.05384421]]\n"
     ]
    }
   ],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "\n",
    "class EmbedNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_embeddings,\n",
    "        embedding_dim,\n",
    "        num_numeric_features,\n",
    "        num_outputs,\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "    ):\n",
    "        super(EmbedNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        total_feature_dim = (\n",
    "            embedding_dim + num_numeric_features\n",
    "        )  # Total input dimension for the first hidden layer\n",
    "\n",
    "        # First layer after concatenation needs to handle total combined dimensions\n",
    "        self.fc1 = nn.Linear(total_feature_dim, hidden_size)\n",
    "        self.fc_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_size, hidden_size) for _ in range(num_layers - 1)]\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Assuming the first column of X is for embedding, and the rest are numerical features\n",
    "        x_cat = X[:, 0].long()  # Categorical features for embedding\n",
    "        x_num = X[:, 1:]  # Numerical features\n",
    "\n",
    "        x_embed = self.embedding(x_cat).view(\n",
    "            x_cat.size(0), -1\n",
    "        )  # Reshape embedded output\n",
    "        x_combined = torch.cat(\n",
    "            [x_embed, x_num], dim=1\n",
    "        )  # Concatenate embeddings with numerical features\n",
    "\n",
    "        x = relu(self.fc1(x_combined))\n",
    "        for layer in self.fc_layers:\n",
    "            x = relu(layer(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    module=EmbedNet,\n",
    "    module__num_embeddings=10,\n",
    "    module__embedding_dim=5,\n",
    "    module__num_numeric_features=8,\n",
    "    module__num_outputs=1,\n",
    "    criterion=torch.nn.MSELoss,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer__lr=0.01,\n",
    "    max_epochs=10,\n",
    "    batch_size=128,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "# Assuming X_cat and X_num are previously defined torch tensors\n",
    "X_cat = torch.randint(\n",
    "    0, 10, (100000, 1), dtype=torch.long\n",
    ")  # Random indices for embedding\n",
    "X_num = torch.randn(100000, 8)  # Random numerical features\n",
    "X = torch.cat((X_cat, X_num), dim=1).numpy()  # Convert to numpy and concatenate\n",
    "y = torch.randn(100000, 1).numpy()  # Random targets\n",
    "\n",
    "# Fit the model\n",
    "net.fit(X, y)\n",
    "\n",
    "# Predict using the trained model\n",
    "y_pred = net.predict(X[:5])\n",
    "print(y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m# Prepare the network and data\u001b[39;00m\n\u001b[1;32m     94\u001b[0m categorical_columns \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcat1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcat2\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 95\u001b[0m X_prepared \u001b[39m=\u001b[39m prepare_data(X\u001b[39m.\u001b[39;49mcopy(), categorical_columns)\n\u001b[1;32m     97\u001b[0m \u001b[39m# Convert DataFrame to numpy array\u001b[39;00m\n\u001b[1;32m     98\u001b[0m X_np \u001b[39m=\u001b[39m X_prepared\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n",
      "Cell \u001b[0;32mIn[2], line 89\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m(X, categorical_columns)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_data\u001b[39m(X, categorical_columns):\n\u001b[1;32m     87\u001b[0m     \u001b[39m# Convert categorical columns to category codes\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m categorical_columns:\n\u001b[0;32m---> 89\u001b[0m         X[col] \u001b[39m=\u001b[39m X[col]\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcat\u001b[39m.\u001b[39mcodes\n\u001b[1;32m     90\u001b[0m     \u001b[39mreturn\u001b[39;00m X\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "\n",
    "class EmbedNet(nn.Module):\n",
    "    def __init__(self, input_dim, categorical_indices, categorical_sizes):\n",
    "        super(EmbedNet, self).__init__()\n",
    "        # Embedding layers for categorical features\n",
    "        self.embeddings = nn.ModuleList(\n",
    "            [\n",
    "                nn.Embedding(\n",
    "                    num_embeddings=size, embedding_dim=min(50, (size + 1) // 2)\n",
    "                )\n",
    "                for size in categorical_sizes\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        num_embeddings_output = sum(e.embedding_dim for e in self.embeddings)\n",
    "        num_numeric_features = input_dim - len(categorical_indices)\n",
    "        total_feature_dim = num_embeddings_output + num_numeric_features\n",
    "\n",
    "        # Network layers\n",
    "        hidden_size = 128\n",
    "        self.fc1 = nn.Linear(total_feature_dim, hidden_size)\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.output = nn.Linear(hidden_size, 1)  # Assuming regression\n",
    "\n",
    "    def forward(self, X):\n",
    "        x_cat = [X[:, i].long() for i in range(len(self.embeddings))]\n",
    "        x_num = X[:, len(self.embeddings) :]\n",
    "\n",
    "        x_embed = [embedding(cat) for cat, embedding in zip(x_cat, self.embeddings)]\n",
    "        x_embed = (\n",
    "            torch.cat(x_embed, dim=1)\n",
    "            if x_embed\n",
    "            else torch.empty(X.size(0), 0).to(X.device)\n",
    "        )\n",
    "\n",
    "        # Ensure numerical features are the correct tensor type\n",
    "        x_num = x_num.float()\n",
    "\n",
    "        # Check dimensions before concatenation\n",
    "        # print(\"x_embed shape:\", x_embed.shape)\n",
    "        # print(\"x_num shape:\", x_num.shape)\n",
    "\n",
    "        x_combined = torch.cat([x_embed, x_num], dim=1)\n",
    "        x = self.fc1(x_combined)\n",
    "        x = self.fc_layers(x)\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "# Function to prepare the network with dynamic features\n",
    "def prepare_network(X, y):\n",
    "    categorical_features = [\n",
    "        X.columns.get_loc(col) for col in X.select_dtypes(include=[\"category\"]).columns\n",
    "    ]\n",
    "    categorical_sizes = [\n",
    "        X[col].nunique() for col in X.select_dtypes(include=[\"category\"]).columns\n",
    "    ]\n",
    "\n",
    "    net = NeuralNetRegressor(\n",
    "        module=EmbedNet,\n",
    "        module__input_dim=X.shape[1],\n",
    "        module__categorical_indices=categorical_features,\n",
    "        module__categorical_sizes=categorical_sizes,\n",
    "        criterion=torch.nn.MSELoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        optimizer__lr=0.01,\n",
    "        max_epochs=10,\n",
    "        batch_size=32,\n",
    "        train_split=None,  # Disable cross-validation\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "def prepare_data(X, categorical_columns):\n",
    "    # Convert categorical columns to category codes\n",
    "    for col in categorical_columns:\n",
    "        X[col] = X[col].astype(\"category\").cat.codes\n",
    "    return X\n",
    "\n",
    "\n",
    "# Prepare the network and data\n",
    "categorical_columns = [\"cat1\", \"cat2\"]\n",
    "X_prepared = prepare_data(X.copy(), categorical_columns)\n",
    "\n",
    "# Convert DataFrame to numpy array\n",
    "X_np = X_prepared.values.astype(np.float32)\n",
    "y_np = y.values.astype(np.float32)\n",
    "\n",
    "# Initialize and fit the model\n",
    "net = prepare_network(X_prepared, y)\n",
    "net.fit(X_np, y_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column cat1: Maximum index found: 3\n",
      "Column cat2: Maximum index found: 2\n",
      "Input tensor shape: torch.Size([4, 3])\n",
      "Error during training: index out of range in self\n",
      "df:   cat1      num1 cat2    target\n",
      "0    a -2.286832    1 -0.459186\n",
      "1    b  0.134887    2  0.086403\n",
      "2    a  0.053830    1 -0.330259\n",
      "3    c  0.183663    2  0.006849\n",
      "Input shapes: (4, 3)\n",
      "Sample inputs: [[ 1.         -2.286832    1.        ]\n",
      " [ 2.          0.13488741  2.        ]\n",
      " [ 1.          0.05382965  1.        ]\n",
      " [ 3.          0.18366256  2.        ]]\n",
      "Target shapes: (4,)\n",
      "Sample targets: [-0.4591855   0.08640274 -0.3302594   0.00684865]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EmbedNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        num_outputs,\n",
    "        categorical_info=None,\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        activation=\"relu\",\n",
    "        use_batchnorm=False,\n",
    "        dropout_prob=0.1,\n",
    "        y_range=None,\n",
    "        problem_type=\"regression\",\n",
    "    ):\n",
    "        super(EmbedNet, self).__init__()\n",
    "        self.problem_type = problem_type\n",
    "        self.y_range = y_range\n",
    "        self.y_constraint = None\n",
    "        self.setup_y_constraints()\n",
    "\n",
    "        # Setup embedding layers if categorical info is provided\n",
    "        self.embeddings = nn.ModuleList()\n",
    "        input_size = (\n",
    "            input_dims - len(categorical_info) if categorical_info else input_dims\n",
    "        )\n",
    "        if categorical_info:\n",
    "            for num_categories, embedding_size in categorical_info:\n",
    "                self.embeddings.append(nn.Embedding(num_categories, embedding_size))\n",
    "                input_size += embedding_size  # Adjust input size based on embeddings\n",
    "\n",
    "        # Setup network layers\n",
    "        layers = []\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm1d(input_size))\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(self.get_activation_fn(activation)())\n",
    "        for _ in range(num_layers - 1):\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(self.get_activation_fn(activation)())\n",
    "        layers.append(nn.Linear(hidden_size, num_outputs))\n",
    "        self.main_block = nn.Sequential(*layers)\n",
    "\n",
    "    def get_activation_fn(self, name):\n",
    "        activations = {\n",
    "            \"relu\": nn.ReLU,\n",
    "            \"elu\": nn.ELU,\n",
    "            \"tanh\": nn.Tanh,\n",
    "            \"identity\": nn.Identity,\n",
    "        }\n",
    "        return activations.get(name, nn.ReLU)\n",
    "\n",
    "    def setup_y_constraints(self):\n",
    "        if self.y_range:\n",
    "            if self.y_range[0] == -float(\"inf\") and self.y_range[1] == float(\"inf\"):\n",
    "                self.y_constraint = None\n",
    "            elif self.y_range[0] >= 0 and self.y_range[1] == float(\"inf\"):\n",
    "                self.y_constraint = \"nonnegative\"\n",
    "            elif self.y_range[0] == -float(\"inf\") and self.y_range[1] <= 0:\n",
    "                self.y_constraint = \"nonpositive\"\n",
    "            else:\n",
    "                self.y_constraint = \"bounded\"\n",
    "                self.y_lower, self.y_upper = self.y_range\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input tensor shape:\", x.shape)  # Add debugging \n",
    "        # Handle categorical features\n",
    "        if self.embeddings:\n",
    "            x_cat = [embed(x[:, i].long()) for i, embed in enumerate(self.embeddings)]\n",
    "            print(\"Embedded outputs shapes:\", [t.shape for t in x_cat])  # Add debugging\n",
    "            x = torch.cat(x_cat + [x[:, len(self.embeddings) :]], dim=1)\n",
    "        print(\"After embedding/concatenation:\", x.shape) # Add debugging\n",
    "        x = self.main_block(x)\n",
    "        if self.problem_type == \"regression\" and self.y_constraint:\n",
    "            x = self.apply_y_constraint(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "    def apply_y_constraint(self, x):\n",
    "        if self.y_constraint == \"nonnegative\":\n",
    "            return torch.abs(x)\n",
    "        elif self.y_constraint == \"nonpositive\":\n",
    "            return -torch.abs(x)\n",
    "        elif self.y_constraint == \"bounded\":\n",
    "            return torch.sigmoid(x) * (self.y_upper - self.y_lower) + self.y_lower\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "# Example DataFrame with mixed types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_data(X, categorical_columns, max_indices=None): \n",
    "    for col in categorical_columns:\n",
    "        X[col] = X[col].astype(\"category\").cat.codes + 1\n",
    "        if max_indices and col in max_indices:\n",
    "            print(f\"Column {col}: Maximum index found: {X[col].max()}\")\n",
    "    return X\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"cat1\": pd.Categorical([\"a\", \"b\", \"a\", \"c\"]),\n",
    "        \"num1\": np.random.randn(4),\n",
    "        \"cat2\": pd.Categorical([1, 2, 1, 2]),\n",
    "        \"target\": np.random.randn(4),\n",
    "    }\n",
    ")\n",
    "\n",
    "X = data.drop(columns=[\"target\"])\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Convert DataFrame to numpy array after encoding categorical features\n",
    "categorical_columns = [\"cat1\", \"cat2\"]\n",
    "max_indices = {\n",
    "    \"cat1\": 2,  # Assuming the embedding layer for \"cat1\" can handle indices 0, 1, 2\n",
    "    \"cat2\": 1   # Assuming the embedding layer for \"cat2\" can handle indices 0, 1\n",
    "}\n",
    "\n",
    "# Prepare the data\n",
    "X_prepared = prepare_data(X.copy(), categorical_columns, max_indices)\n",
    "\n",
    "# Convert DataFrame to numpy array\n",
    "X_np = X_prepared.values.astype(np.float32)\n",
    "y_np = y.values.astype(np.float32)\n",
    "\n",
    "\n",
    "def prepare_network(X, num_outputs):\n",
    "    categorical_info = []\n",
    "    for col in X.select_dtypes(include=[\"category\"]).columns:\n",
    "        max_index = X[col].cat.codes.max()  # Get the maximum index from codes\n",
    "        num_categories = max_index + 1  # Account for the number of unique categories\n",
    "        embedding_size = min(50, (num_categories + 1) // 2)  # Determine the size of embedding\n",
    "        categorical_info.append((num_categories, embedding_size))\n",
    "        \n",
    "    net = NeuralNetRegressor(\n",
    "        module=EmbedNet,\n",
    "        module__input_dims=X.shape[1],\n",
    "        module__num_outputs=num_outputs,\n",
    "        module__categorical_info=categorical_info,\n",
    "        criterion=torch.nn.MSELoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        optimizer__lr=0.01,\n",
    "        max_epochs=10,\n",
    "        batch_size=32,\n",
    "        train_split=None,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    return net\n",
    "\n",
    "\n",
    "# Initialize and fit the model\n",
    "net = prepare_network(X, 1)  # Assuming 'target' is a single output scenario\n",
    "try:\n",
    "    net.fit(X_np, y_np)\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    # Optionally print more diagnostic information\n",
    "    print(\"df:\", data)\n",
    "    print(\"Input shapes:\", X_np.shape)\n",
    "    print(\"Sample inputs:\", X_np[:5])\n",
    "    print(\"Target shapes:\", y_np.shape)\n",
    "    print(\"Sample targets:\", y_np[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column cat1: Maximum index found: 3\n",
      "Column cat2: Maximum index found: 2\n",
      "Input tensor shape: torch.Size([4, 3])\n",
      "Error during training: index out of range in self\n",
      "df:   cat1      num1 cat2    target\n",
      "0    a -2.286832    1 -0.459186\n",
      "1    b  0.134887    2  0.086403\n",
      "2    a  0.053830    1 -0.330259\n",
      "3    c  0.183663    2  0.006849\n",
      "Input shapes: (4, 3)\n",
      "Sample inputs: [[ 1.         -2.286832    1.        ]\n",
      " [ 2.          0.13488741  2.        ]\n",
      " [ 1.          0.05382965  1.        ]\n",
      " [ 3.          0.18366256  2.        ]]\n",
      "Target shapes: (4,)\n",
      "Sample targets: [-0.4591855   0.08640274 -0.3302594   0.00684865]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_embed_sizes(train_dataset, params, num_categs_per_feature):\n",
    "    \"\"\"Returns list of embedding sizes for each categorical variable.\n",
    "    Selects this adaptively based on training_dataset.\n",
    "    Note: Assumes there is at least one embed feature.\n",
    "    \"\"\"\n",
    "    max_embedding_dim = params[\"max_embedding_dim\"]\n",
    "    embed_exponent = params[\"embed_exponent\"]\n",
    "    size_factor = params[\"embedding_size_factor\"]\n",
    "    embed_dims = [\n",
    "        int(size_factor * max(2, min(max_embedding_dim, 1.6 * num_categs_per_feature[i] ** embed_exponent))) for i in range(len(num_categs_per_feature))\n",
    "    ]\n",
    "    return embed_dims\n",
    "\n",
    "class EmbedNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dims,\n",
    "        num_outputs,\n",
    "        categorical_info=None,\n",
    "        hidden_size=128,\n",
    "        num_layers=2,\n",
    "        activation=\"relu\",\n",
    "        use_batchnorm=False,\n",
    "        dropout_prob=0.1,\n",
    "        y_range=None,\n",
    "        problem_type=\"regression\",\n",
    "    ):\n",
    "        super(EmbedNet, self).__init__()\n",
    "        self.problem_type = problem_type\n",
    "        self.y_range = y_range\n",
    "        self.y_constraint = None\n",
    "        self.setup_y_constraints()\n",
    "\n",
    "        # Setup embedding layers if categorical info is provided\n",
    "        self.embeddings = nn.ModuleList()\n",
    "        input_size = (\n",
    "            input_dims - len(categorical_info) if categorical_info else input_dims\n",
    "        )\n",
    "        if categorical_info:\n",
    "            for num_categories, embedding_size in categorical_info:\n",
    "                self.embeddings.append(nn.Embedding(num_categories, embedding_size))\n",
    "                input_size += embedding_size  # Adjust input size based on embeddings\n",
    "\n",
    "        # Setup network layers\n",
    "        layers = []\n",
    "        if use_batchnorm:\n",
    "            layers.append(nn.BatchNorm1d(input_size))\n",
    "        layers.append(nn.Linear(input_size, hidden_size))\n",
    "        layers.append(self.get_activation_fn(activation)())\n",
    "        for _ in range(num_layers - 1):\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "            layers.append(nn.Dropout(dropout_prob))\n",
    "            layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "            layers.append(self.get_activation_fn(activation)())\n",
    "        layers.append(nn.Linear(hidden_size, num_outputs))\n",
    "        self.main_block = nn.Sequential(*layers)\n",
    "\n",
    "    def get_activation_fn(self, name):\n",
    "        activations = {\n",
    "            \"relu\": nn.ReLU,\n",
    "            \"elu\": nn.ELU,\n",
    "            \"tanh\": nn.Tanh,\n",
    "            \"identity\": nn.Identity,\n",
    "        }\n",
    "        return activations.get(name, nn.ReLU)\n",
    "\n",
    "    def setup_y_constraints(self):\n",
    "        if self.y_range:\n",
    "            if self.y_range[0] == -float(\"inf\") and self.y_range[1] == float(\"inf\"):\n",
    "                self.y_constraint = None\n",
    "            elif self.y_range[0] >= 0 and self.y_range[1] == float(\"inf\"):\n",
    "                self.y_constraint = \"nonnegative\"\n",
    "            elif self.y_range[0] == -float(\"inf\") and self.y_range[1] <= 0:\n",
    "                self.y_constraint = \"nonpositive\"\n",
    "            else:\n",
    "                self.y_constraint = \"bounded\"\n",
    "                self.y_lower, self.y_upper = self.y_range\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Input tensor shape:\", x.shape)  # Add debugging \n",
    "        # Handle categorical features\n",
    "        if self.embeddings:\n",
    "            x_cat = [embed(x[:, i].long()) for i, embed in enumerate(self.embeddings)]\n",
    "            print(\"Embedded outputs shapes:\", [t.shape for t in x_cat])  # Add debugging\n",
    "            x = torch.cat(x_cat + [x[:, len(self.embeddings) :]], dim=1)\n",
    "        print(\"After embedding/concatenation:\", x.shape) # Add debugging\n",
    "        x = self.main_block(x)\n",
    "        if self.problem_type == \"regression\" and self.y_constraint:\n",
    "            x = self.apply_y_constraint(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "    def apply_y_constraint(self, x):\n",
    "        if self.y_constraint == \"nonnegative\":\n",
    "            return torch.abs(x)\n",
    "        elif self.y_constraint == \"nonpositive\":\n",
    "            return -torch.abs(x)\n",
    "        elif self.y_constraint == \"bounded\":\n",
    "            return torch.sigmoid(x) * (self.y_upper - self.y_lower) + self.y_lower\n",
    "        return x\n",
    "\n",
    "\n",
    "import torch\n",
    "from skorch import NeuralNetRegressor\n",
    "\n",
    "# Example DataFrame with mixed types\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def prepare_data(X, categorical_columns, max_indices=None): \n",
    "    for col in categorical_columns:\n",
    "        X[col] = X[col].astype(\"category\").cat.codes + 1\n",
    "        if max_indices and col in max_indices:\n",
    "            print(f\"Column {col}: Maximum index found: {X[col].max()}\")\n",
    "    return X\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"cat1\": pd.Categorical([\"a\", \"b\", \"a\", \"c\"]),\n",
    "        \"num1\": np.random.randn(4),\n",
    "        \"cat2\": pd.Categorical([1, 2, 1, 2]),\n",
    "        \"target\": np.random.randn(4),\n",
    "    }\n",
    ")\n",
    "\n",
    "X = data.drop(columns=[\"target\"])\n",
    "y = data[\"target\"]\n",
    "\n",
    "# Convert DataFrame to numpy array after encoding categorical features\n",
    "categorical_columns = [\"cat1\", \"cat2\"]\n",
    "max_indices = {\n",
    "    \"cat1\": 2,  # Assuming the embedding layer for \"cat1\" can handle indices 0, 1, 2\n",
    "    \"cat2\": 1   # Assuming the embedding layer for \"cat2\" can handle indices 0, 1\n",
    "}\n",
    "\n",
    "# Prepare the data\n",
    "X_prepared = prepare_data(X.copy(), categorical_columns, max_indices)\n",
    "\n",
    "# Convert DataFrame to numpy array\n",
    "X_np = X_prepared.values.astype(np.float32)\n",
    "y_np = y.values.astype(np.float32)\n",
    "\n",
    "\n",
    "def prepare_network(X, num_outputs):\n",
    "    categorical_info = []\n",
    "    for col in X.select_dtypes(include=[\"category\"]).columns:\n",
    "        max_index = X[col].cat.codes.max()  # Get the maximum index from codes\n",
    "        num_categories = max_index + 1  # Account for the number of unique categories\n",
    "        embedding_size = min(50, (num_categories + 1) // 2)  # Determine the size of embedding\n",
    "        categorical_info.append((num_categories, embedding_size))\n",
    "        \n",
    "    net = NeuralNetRegressor(\n",
    "        module=EmbedNet,\n",
    "        module__input_dims=X.shape[1],\n",
    "        module__num_outputs=num_outputs,\n",
    "        module__categorical_info=categorical_info,\n",
    "        criterion=torch.nn.MSELoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        optimizer__lr=0.01,\n",
    "        max_epochs=10,\n",
    "        batch_size=32,\n",
    "        train_split=None,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "    return net\n",
    "\n",
    "\n",
    "# Initialize and fit the model\n",
    "net = prepare_network(X, 1)  # Assuming 'target' is a single output scenario\n",
    "try:\n",
    "    net.fit(X_np, y_np)\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    # Optionally print more diagnostic information\n",
    "    print(\"df:\", data)\n",
    "    print(\"Input shapes:\", X_np.shape)\n",
    "    print(\"Sample inputs:\", X_np[:5])\n",
    "    print(\"Target shapes:\", y_np.shape)\n",
    "    print(\"Sample targets:\", y_np[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
